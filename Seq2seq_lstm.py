# -*- coding: utf-8 -*-
"""12. seq2seq-LSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18LbVEBzZucC4S68URChB8iidpvlN5oIL

## Install modules
"""

!pip install konlpy

"""## Connect my Google drive, Import modules"""

# Connect my Google drive
from google.colab import drive

drive.mount('/gdrive')
gdrive_root = '/gdrive/My Drive/Research/programming/English2Korean dataset' # 폴더 위치 생각하고 연결하기

# Import modules
import os

import numpy as np
import easydict
import pandas as pd
import pickle
import random
import spacy as sp # 자연어 처리를 위한 library
from konlpy.tag import Okt # 한국어는 spacy에서 token화를 지원하지 않기에 konlpy 사용
from torchtext.legacy.data import Field, BucketIterator# 전처리를 위한 Field, batch+padding을 위한 bi

import torch
import torch.optim as optim
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

device = 'cuda'
torch.manual_seed(1) 
torch.cuda.manual_seed(1)

"""## Data preprocessing"""

'''
# data 불러오기
data_root1 = gdrive_root + '/1_구어체.xlsx'
data_root2 = gdrive_root + '/2_대화체.xlsx'
data_root3 = gdrive_root + '/3_문어체_뉴스.xlsx'
data_root4 = gdrive_root + '/4_문어체_한국문화.xlsx'
data_root5 = gdrive_root + '/5_문어체_조례.xlsx'
data_root6 = gdrive_root + '/6_문어체_지자체웹사이트.xlsx'

# 각 excel별로 정보의 종류와 양이 달라 각각 전처리후 결합
df1 = pd.read_excel(data_root1)
df2 = pd.read_excel(data_root2)
df3 = pd.read_excel(data_root3)
df4 = pd.read_excel(data_root4)
df5 = pd.read_excel(data_root5)
df6 = pd.read_excel(data_root6)

df1 = df1.drop(['영어', 'Temp'], axis=1)
df2 = df2.drop(['대분류', '소분류', '상황', '발화자', 'Set Nr.', '영어'], axis=1)
df2 = df2.rename(columns = {'영어검수':'영어 검수'})
df3 = df3.drop(['ID', '날짜', '자동분류_1', '자동분류_2', '자동분류_3', 'URL', '언론사', 'NMT', 'PE'], axis=1)
df3 = df3.rename(columns = {'원문': '한국어', 'REVIEW':'영어 검수'})
df4 = df4.drop(['ID', '키워드', 'NMT', 'PE'], axis=1)
df4 = df4.rename(columns = {'원문': '한국어', 'Review':'영어 검수'})
df5 = df5.drop(['ID', '키워드', 'NMT', 'PE'], axis=1)
df5 = df5.rename(columns = {'원문': '한국어', 'Review':'영어 검수'})
df6 = df6.drop(['ID','지자체', '제목','URL', 'NMT', 'PE'], axis=1)
df6 = df6.rename(columns = {'원문': '한국어', 'Review':'영어 검수'})

# 전체 data를 한 dict에 모으기
full_df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)
dict_data = full_df.to_dict()

# tokenize 도구 초기화
okt = Okt()
spacy_en = sp.load('en')

# 영어 문장을 tokenize + reverse하는 함수
def tokenize_en(text):
  return [token.text for token in spacy_en.tokenizer(text)][::-1]

# 한국어 문장을 tokenize하는 함수
def tokenize_ko(text):
  return [token for token in okt.morphs(text)]

# dict data를 tokenize function을 적용한 후 저장 / 전처리 완료
full_data_dict = {}
a = []
b = []
c = []
d = []
for i in range(len(dict_data['한국어'])):
  a = tokenize_ko(dict_data['한국어'][i])
  b.append(a)
  c = tokenize_en(dict_data['영어 검수'][i])
  d.append(c)
full_data_dict['한국어'] = b
full_data_dict['영어'] = d

print(len(full_data_dict['한국어']))
print(len(full_data_dict['영어']))

data_dir = os.path.join('/gdrive/My Drive/Research/programming/English2Korean dataset', 'pre_processed_dataset')
if not os.path.exists(data_dir):
  os.makedirs(data_dir)

data_path = os.path.join(data_dir, 'pre_processed_dataset')

f = open(data_path, 'wb')
pickle.dump(full_data_dict, f)
f.close()
'''

"""## Load pre-processed dataset"""

# load full_dataset
data_dir = os.path.join('/gdrive/My Drive/Research/programming/English2Korean dataset', 'pre_processed_dataset')
data_path = os.path.join(data_dir, 'pre_processed_dataset')
f = open(data_path, 'rb')
full_dataset = pickle.load(f)

"""## Construct data"""

# field class + build_vocab mothod 사용을 위한 준비
okt = Okt()
spacy_en = sp.load('en')

def tokenize_en(text):
  return [token.text for token in spacy_en.tokenizer(text)][::-1]

def tokenize_ko(text):
  return [token for token in okt.morphs(text)]

# 전처리 내용을 명시하여 전달해주기 위해 field class사용
SRC = Field(tokenize=tokenize_en, init_token="<sos>", eos_token="<eos>", lower=True)
TRG = Field(tokenize=tokenize_ko, init_token="<sos>", eos_token="<eos>", lower=True)

# 단어들로 library 제작
SRC.build_vocab(full_dataset['영어'], min_freq=4)
TRG.build_vocab(full_dataset['한국어'], min_freq=4)

# 제작된 library를 바탕으로 모든 문장의 단어 label화
src_encode_list = []
for i in range(len(full_dataset['영어'])):
  a=[2]
  sentence = full_dataset['영어'][i]
  for i in range(len(sentence)):
    word = str(sentence[i].lower())
    a.append(SRC.vocab.stoi[word])
  a.append(3)
  src_encode_list.append(list(a))

trg_encode_list = []
for i in range(len(full_dataset['한국어'])):
  a=[2]
  sentence = full_dataset['한국어'][i]
  for i in range(len(sentence)):
    word = str(sentence[i].lower())
    a.append(TRG.vocab.stoi[word])
  a.append(3)
  trg_encode_list.append(list(a))

# 인위적이지만 batch size를 14에 맞추어 padding 추가 전처리
s_length = []
t_length = []
repeat = 0

for i in range(4733):
  for i in range(14):
    a = len(src_encode_list[i + 14*repeat])
    s_length.append(a)
    b = len(trg_encode_list[i + 14*repeat])
    t_length.append(b)

  s_m = max(s_length)
  t_m = max(t_length)

  for i in range(14):
    s_gap = s_m - len(src_encode_list[i + 14*repeat])
    t_gap = t_m - len(trg_encode_list[i + 14*repeat])
    if s_gap != 0:  
      src_encode_list[i + 14*repeat] += [1]*s_gap
    else:
      pass
    if t_gap != 0:
      trg_encode_list[i + 14*repeat] += [1]*t_gap
    else:
      pass

  repeat += 1

"""## Constuct DataLoader"""

class trainDataset(Dataset):
  def __init__(self, src_encode_list, trg_encode_list):
    super(trainDataset, self).__init__()
    self.src_data = src_encode_list
    self.trg_data = trg_encode_list

  def __len__(self):
    return len(self.src_data)

  def __getitem__(self, idx):
    src_sen = self.src_data[idx]
    trg_sen = self.trg_data[idx]

    src_sen = torch.tensor(src_sen)
    trg_sen = torch.tensor(trg_sen)

    full_sen = {'src' : src_sen, 'trg' : trg_sen}

    return full_sen

train_dataset = trainDataset(src_encode_list, trg_encode_list)
train_dataloader = DataLoader(train_dataset, batch_size = 14, shuffle = False)

"""## Utils"""

args = easydict.EasyDict({'en_vocab' : len(SRC.vocab),
                          'ko_vocab' : len(TRG.vocab),
                          'embed_dim' : 256,
                          'drop_out_ratio' : 0.5,
                          'hidden_size' : 512,
                          'num_layers' : 2,
                          'learning_rate' : 0.001,
                          'num_epoch' : 30
                          })

"""## Construct network"""

class Encoder(nn.Module):
  def __init__(self, args):
    super(Encoder, self).__init__()

    # pytorch의 경우 word data를 one-hot vector로 바꾸는 것이 아닌 정수 index로 넣어도 table을 만들어준다.
    self.embedding = nn.Embedding(num_embeddings=args.en_vocab, embedding_dim = args.embed_dim)
    self.lstm = nn.LSTM(input_size=args.embed_dim, hidden_size=args.hidden_size, 
                        num_layers=args.num_layers, dropout=args.drop_out_ratio)
    self.dropout = nn.Dropout(args.drop_out_ratio)
    
  def forward(self, src):
    embedded = self.dropout(self.embedding(src))
    outputs, (hidden, cell) = self.lstm(embedded)

    return hidden, cell

class Decoder(nn.Module):
  def __init__(self, args):
    super(Decoder, self).__init__()

    self.embedding = nn.Embedding(num_embeddings=args.ko_vocab, embedding_dim = args.embed_dim)
    self.lstm = nn.LSTM(input_size=args.embed_dim, hidden_size=args.hidden_size, 
                        num_layers=args.num_layers, dropout=args.drop_out_ratio)
    self.dropout = nn.Dropout(args.drop_out_ratio)
    # translation 값을 내는 부분 
    self.output_dim = args.ko_vocab
    self.fc_out = nn.Linear(in_features=args.hidden_size, out_features=args.ko_vocab)

  def forward(self, input, hidden, cell):

    embedded = self.dropout(self.embedding(input))
    output, (hidden, cell) = self.lstm(embedded)
    prediction = self.fc_out(output.squeeze(1)) 

    return prediction, hidden, cell

class Seq2Seq(nn.Module):
  def __init__(self, encoder, decoder, device):
    super(Seq2Seq, self).__init__()
    self.encoder = encoder
    self.decoder = decoder
    self.device = device

  def forward(self, src, trg, teacher_forcing_ratio=0.5):
    # encoding 과정
    hidden, cell = self.encoder(src)
    # decoder의 결과물을 담을 tensor 객체 제작
    batch_size = trg.shape[0]
    trg_len = trg.shape[1]
    trg_vocab_size = self.decoder.output_dim
    outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device) # shape = (14, len)
    # decoding 과정
    input = trg[:, 0] # input은 sos token으로 고정
    input = input.unsqueeze(1)
    for i in range(1, trg_len):
      prediction, hidden, cell = self.decoder(input, hidden, cell)
      outputs[:, i] = prediction # shape = (14, 18521)
      top1 = outputs[:, i].max(dim=1).indices # 각 prediction 배열에서 가장 높은 값의 index찾기
      # 입력의 일부를 무조건 정답으로 넣어줘 초기 학습 속도를 올린다.(정답의 비율이 teacher_forcing_ratio)
      teacher_force = random.random() < teacher_forcing_ratio # bool형 return
      input = trg[:, i] if teacher_force else top1
      input = input.unsqueeze(1)

    return outputs

"""## Initialize the network and optimizer"""

enc = Encoder(args)
dec = Decoder(args)
model = Seq2Seq(enc, dec, device).to(device)

# 모든 하위 module의 weight 초기화 / 있는 code 그대로 사용
def init_weights(m):
  for name, param in m.named_parameters():
    nn.init.uniform_(param.data, -0.08, -0.08)
    
model.apply(init_weights)
optimizer = optim.Adam(model.parameters(), lr = args.learning_rate)

"""## Train the network"""

TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token] # output의 padding을 출력에서 무시하기 위해 사용
loss_function = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)
epoch_loss = 0
#for epoch in range(args.num_epoch):
  #model.train()
for batch, data in enumerate(train_dataloader): 
  src = data['src'].to(device)
  trg = data['trg'].to(device)
  optimizer.zero_grad()
  output = model(src, trg)
  output_dim = output.shape
  # 출력의 첫 index는 sos이기에 사용하지 않음
  output = output[: , 1:].reshape(-1, output_dim)
  trg = trg[:, 1:].reshape(-1)
  print(output.shape)
  print(trg.shape)
  loss = loss_function(output, trg)
  loss.backward()
  optimizer.step
  epoch_loss += loss.item()
  print(epoch_loss)
  break

loss = nn.CrossEntropyLoss()
input = torch.randn(3, 5, requires_grad=True)
target = torch.empty(3, dtype=torch.long).random_(5)
print(input.shape)
print(target.shape)
output = loss(input, target)
output.backward()

