# -*- coding: utf-8 -*-
"""10. GAN-Cycle GAN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k74sre5s94nHYeqiFLIL7ElbtSqSIOID

# 해결해야할 문제
1. transform적용

## Connect my Google drive, Import modules
"""

# Connect my Google drive
from google.colab import drive

drive.mount('/gdrive')
root = '/gdrive/My Drive/Research/programming/Cycle_GAN_dataset'

# Import modules

import os

import numpy as np
import PIL
from PIL import Image
import matplotlib.pyplot as plt
import glob
import easydict
import itertools
import random

import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import torch.autograd
import torchvision
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

device = 'cuda'
torch.manual_seed(1) 
torch.cuda.manual_seed(1)

"""## Construct data"""

# 속성으로 method처럼 쓸 수 있는 dict생성(argument parser)
args = easydict.EasyDict({"batch_size" : 1,
                          "photo_folder" : os.path.join(root + '/Photo'),
                          "monet_folder" : os.path.join(root + '/Monet'),
                          "num_epoch" : 50,
                          "learning_rate" : 0.0002,
                          "loss_weight" : 10})

class trainDataset(Dataset):
  
  # 이미지들의 list 추출 / 4610, 1072개
  def __init__(self, args, transform=None):
    super(trainDataset, self).__init__()

    self.photo_list = glob.glob(args.photo_folder + '/*.jpg')
    self.photo_list.sort()
    #self.photo_list = random.shuffle(self.photo_list)
    
    self.monet_list = glob.glob(args.monet_folder + '/*.jpg')
    self.monet_list.sort()
    self.transfrom = transform 

  def __len__(self):

    return len(self.monet_list)

  # 이미지를 넘파이 배열로 읽어들임 / 256, 256, 3
  def __getitem__(self, idx):

    photo_sample = plt.imread(self.photo_list[idx])
    monet_sample = plt.imread(self.monet_list[idx])

    photo_sample = photo_sample.astype(np.float32)
    monet_sample = monet_sample.astype(np.float32)

    if photo_sample.dtype == np.float32:
      photo_sample = photo_sample / 255.0

    if monet_sample.dtype == np.float32:
      monet_sample = monet_sample / 255.0

    photo_sample = torch.tensor(photo_sample)
    monet_sample = torch.tensor(monet_sample)

    photo_sample =  photo_sample.permute(2, 0, 1)
    monet_sample =  monet_sample.permute(2, 0, 1)

    sample = {'photo': photo_sample, 'monet': monet_sample}

    return sample

transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                                transforms.RandomHorizontalFlip(),
                                ])
train_data = trainDataset(args, transform=transform)

train_dataloader = DataLoader(train_data,
                              batch_size = args.batch_size,
                              num_workers=1,
                              shuffle = True)

"""## Construct networks

"""

class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    self.c7s1_64 = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=1, padding=3,
                                           padding_mode='reflect'),
                                 nn.InstanceNorm2d(64), # 1개 batch 내에서 channel 별로 norm
                                 nn.ReLU()
                                 )
    self.d_layer = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1,
                                           padding_mode='reflect'), # 128x128
                                 nn.InstanceNorm2d(128),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1,
                                           padding_mode='reflect'), # 64x64
                                 nn.InstanceNorm2d(256),
                                 nn.ReLU()
                                 )
    self.r_layer = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,
                                           padding_mode='reflect'),
                                 nn.InstanceNorm2d(256),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1,
                                           padding_mode='reflect'),
                                 nn.InstanceNorm2d(256)
                                 )
    self.u_layer = nn.Sequential(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, 
                                                    padding=1),
                                 nn.InstanceNorm2d(128),
                                 nn.ReLU(),
                                 nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, 
                                                    padding=0, output_padding=(1, 1)),
                                 nn.InstanceNorm2d(64),
                                 nn.ReLU(),
                                 )
    
    self.c7s1_3 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=7, stride=1, padding=3, padding_mode='reflect')
                                 
                                 

  def forward(self, x):
    x = self.c7s1_64(x)
    x = self.d_layer(x)
    
    for i in range(9):
      x1 = x
      x = self.r_layer(x)
      x += x1
      x = F.relu(x)
      
    x = self.u_layer(x)
    x = self.c7s1_3(x)
    x = torch.tanh(x) # 학습이 원활하게 진행되게 하기 위하여 사용 / output을 [-1, 1] 사이에 있게 하기 위함
    
    return x

# 70x70 patch GAN / 70x70 단위(correlation의 관계가 유지되는 선-hyper_parameter)로 generator의 진위 확인
class Discriminator(nn.Module): 
  def __init__(self):
    super(Discriminator, self).__init__()
    self.c64_layer = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1,
                                           padding_mode='reflect'),
                                  nn.LeakyReLU(negative_slope=0.2)
                                 )
    self.c128_layer = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1,
                                           padding_mode='reflect'),
                                  nn.InstanceNorm2d(128),
                                  nn.LeakyReLU(negative_slope=0.2)
                                 )
    self.c256_layer = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1,
                                           padding_mode='reflect'),
                                  nn.InstanceNorm2d(256),
                                  nn.LeakyReLU(negative_slope=0.2)
                                 )
    self.c512_layer = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=1, padding=1,
                                           padding_mode='reflect'),
                                  nn.InstanceNorm2d(512),
                                  nn.LeakyReLU(negative_slope=0.2)
                                 )
    self.last_layer = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=1,
                                           padding_mode='reflect')

  def forward(self, x):
    x = self.c64_layer(x)    
    x = self.c128_layer(x)
    x = self.c256_layer(x)
    x = self.c512_layer(x)
    x = self.last_layer(x)

    x = torch.sigmoid(x) # 0~1의 확률 값을 출력하기 위해서 사용
    
    return x

"""## Initialize the network and optimizer"""

# 가중치 초기화를 해야하는데 아직 구현 x
gen_p2m = Generator().to(device)
gen_m2p = Generator().to(device)
m_dis = Discriminator().to(device)
p_dis = Discriminator().to(device)

#parameter를 기억하지 못해서 한번에 합쳐서 update 
optimizerD = optim.Adam(itertools.chain(p_dis.parameters(),m_dis.parameters()), lr = args.learning_rate)
optimizerG = optim.Adam(itertools.chain(gen_m2p.parameters(), gen_p2m.parameters()), lr = args.learning_rate)

"""## Load pre-trained weights if exist"""

lowest_gen_loss = 20
lowest_dis_loss = 20

G_ckpt_dir = os.path.join('/gdrive/My Drive/Research/programming', 'G_checkpoints')
if not os.path.exists(G_ckpt_dir):
  os.makedirs(G_ckpt_dir)

G_ckpt_path = os.path.join(G_ckpt_dir, 'lastest.pt')
if os.path.exists(G_ckpt_path):
  G_ckpt = torch.load(G_ckpt_path)
  try:
    gen_p2m.load_state_dict(G_ckpt['monet_generator'])
    gen_m2p.load_state_dict(G_ckpt['photo_generator'])
    optimizerG.load_state_dict(G_ckpt['optimizer'])
    lowest_gen_loss = G_ckpt['gen_loss']

  except RuntimeError as e:
    print('wrong checkpoint')
  else:
    print('All generator checkpoints are loaded !')
    print('current mgen lowest loss : %.2f' % lowest_gen_loss)

D_ckpt_dir = os.path.join('/gdrive/My Drive/Research/programming', 'D_checkpoints')
if not os.path.exists(D_ckpt_dir):
  os.makedirs(D_ckpt_dir)

D_ckpt_path = os.path.join(D_ckpt_dir, 'lastest.pt')
if os.path.exists(D_ckpt_path):
  D_ckpt = torch.load(D_ckpt_path)
  try:
    m_dis.load_state_dict(D_ckpt['monet_discriminator'])
    p_dis.load_state_dict(D_ckpt['photo_discriminator'])
    optimizerD.load_state_dict(D_ckpt['optimizer'])
    lowest_dis_loss = D_ckpt['dis_loss']

  except RuntimeError as e:
    print('wrong checkpoint')
  else:
    print('All discrimonator checkpoints are loaded !')
    print('current dis lowest loss : %.2f' % lowest_dis_loss)

"""## Train the network"""

def requires_grad(nets, requires_grad=False):
  for net in nets:
    for param in net.parameters():
      param.requires_grad = requires_grad

cycle_loss = nn.L1Loss()
iden_loss = nn.L1Loss()
adver_loss = nn.BCELoss()

mgen_loss = []
pgen_loss = []
mdis_loss = []
pdis_loss = []

for epoch in range(args.num_epoch):
  gen_p2m.train(), gen_m2p.train(), m_dis.train(), p_dis.train()
  for batch, sample in enumerate(train_dataloader):
    photo = sample['photo'].to(device)
    monet = sample['monet'].to(device)

    #forward network
    fake_photo = gen_m2p(monet)
    cons_monet = gen_p2m(fake_photo)

    fake_monet = gen_p2m(photo)
    cons_photo = gen_m2p(fake_monet)

    iden_monet = gen_p2m(monet)
    iden_photo = gen_m2p(photo)

    #backward disriminator
    requires_grad([m_dis, p_dis], True)
    optimizerD.zero_grad()

    m_dis_real = m_dis(monet)
    m_dis_fake = m_dis(fake_monet.detach())
    
    mdis_ad_rloss = adver_loss(m_dis_real, torch.ones_like(m_dis_real))
    mdis_ad_floss = adver_loss(m_dis_fake, torch.zeros_like(m_dis_fake))

    mdis_loss = 0.5 * (mdis_ad_rloss + mdis_ad_floss)

    p_dis_real = p_dis(photo)
    p_dis_fake = p_dis(fake_photo.detach())

    pdis_ad_rloss = adver_loss(p_dis_real, torch.ones_like(p_dis_real))
    pdis_ad_floss = adver_loss(p_dis_fake, torch.zeros_like(p_dis_fake))

    pdis_loss = 0.5 * (pdis_ad_rloss + pdis_ad_floss)

    dis_loss = mdis_loss + pdis_loss

    dis_loss.backward()
    optimizerD.step()

    #backward generator
    requires_grad([m_dis, p_dis], False)
    optimizerG.zero_grad()

    m_dis_fake = m_dis(fake_monet) # update 했으니까 다시 넣기
    p_dis_fake = p_dis(fake_photo) 

    mgen_ad_loss = adver_loss(m_dis_fake, torch.ones_like(m_dis_fake))
    pgen_ad_loss = adver_loss(p_dis_fake, torch.ones_like(p_dis_fake))

    mc_loss = cycle_loss(cons_monet, monet)
    pc_loss = cycle_loss(cons_photo, photo)

    mi_loss = iden_loss(iden_monet, monet)
    pi_loss = iden_loss(iden_photo, photo)

    mgen_loss = (args.loss_weight * mc_loss) + (0.5 * args.loss_weight * mi_loss) + (mgen_ad_loss)
    pgen_loss = (args.loss_weight * pc_loss) + (0.5 * args.loss_weight * pi_loss) + (pgen_ad_loss)

    gen_loss = mgen_loss + pgen_loss

    gen_loss.backward()
    optimizerG.step()

  print('[epoch:{}] D_loss : {:.4f} G_loss : {:.4f}'.format(epoch, dis_loss, gen_loss))

  if lowest_gen_loss > gen_loss:
    lowest_gen_loss = gen_loss

    G_ckpt = {'monet_generator':gen_p2m.state_dict(), 
              'photo_generator':gen_m2p.state_dict(),           
              'optimizer':optimizerG.state_dict(),
              'gen_loss':lowest_gen_loss}


    torch.save(G_ckpt, G_ckpt_path)

    print('generator checkpoint is saved !')

  if lowest_dis_loss > dis_loss:
    lowest_dis_loss = dis_loss

    D_ckpt = {'monet_discriminator':m_dis.state_dict(), 
              'photo_discriminator':p_dis.state_dict(),           
              'optimizer':optimizerD.state_dict(),
              'dis_loss':lowest_dis_loss}


    torch.save(D_ckpt, D_ckpt_path)

    print('discriminator checkpoint is saved !')

"""
## Test generator(photo2monet) and visualization"""

gen_p2m.load_state_dict(G_ckpt['monet_generator'])
torch.cuda.empty_cache()

gen_p2m.eval()

test_folder = os.path.join(root + '/Test_photo')
test_sample = glob.glob(test_folder + '/5.jpg')
test_photo = plt.imread(test_sample[0])

real_test_photo = test_photo

test_photo = test_photo.astype(np.float32)
test_photo = torch.tensor(test_photo)

test_photo = test_photo.permute(2, 0, 1)
test_photo = test_photo.unsqueeze(0)
test_photo = test_photo.to(device)

monet = gen_p2m(test_photo)
monet = monet.permute(0, 2, 3, 1)

#visualization
plt.subplot(1, 2, 1)
photo = plt.imshow(real_test_photo)
plt.subplot(1, 2, 2)
monet = plt.imshow(monet.squeeze().detach().cpu().numpy())