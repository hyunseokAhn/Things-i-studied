# -*- coding: utf-8 -*-
"""7. FCN-U-Net

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eOOlpFjdJnZw_uAlFUjraSfUjDWiOuQw

## Connect my Google drive, Import modules
"""

# Connect my Google drive
from google.colab import drive

drive.mount('/gdrive')
gdrive_root = '/gdrive/My Drive/Research/basic program/7.FCN-U-Net/Isbi_cell_dataset' # 폴더 위치 생각하고 연결하기

# Import modules
import os

import PIL
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import easydict
import glob

import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms, utils
from torchvision.utils import save_image

torch.manual_seed(1) 
torch.cuda.manual_seed(1)

"""## Construct data"""

args = easydict.EasyDict({"batch_size" : 1, # jupyter note와 같은 환경에선 argparse 오류가 발생하여 대체품으로 easydict를 사용했습니다.
                         "volume_folder" : os.path.join(gdrive_root + '/train'),  # 경로를 각 파일의 위치로 변경하였습니다.
                         "label_folder" : os.path.join(gdrive_root + '/label')
                         })

test_args = easydict.EasyDict({"batch_size" : 1, # jupyter note와 같은 환경에선 argparse 오류가 발생하여 대체품으로 easydict를 사용했습니다.
                         "test_folder" : os.path.join(gdrive_root + '/test'),  # 경로를 각 파일의 위치로 변경하였습니다.
                         })

class trainDataset(Dataset):
    
    def __init__ (self, args, transform=None):

        self.volume_list = glob.glob (args.volume_folder + '/*.png')
        self.volume_list.sort()
        self.label_list = glob.glob (args.label_folder + '/*.png')
        self.label_list.sort()
        self.transform = transform
    
    def __len__(self):

        return len(self.label_list)
    
    def __getitem__(self, idx):
        
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        volume_sample = plt.imread (self.volume_list [idx])
        label_sample = plt.imread (self.label_list [idx])
        volume_sample = volume_sample.reshape ((1,)+volume_sample.shape)
        label_sample = label_sample.reshape ((1,)+label_sample.shape)
        
        volume_sample = torch.Tensor(volume_sample)
        label_sample = torch.Tensor (label_sample)
        
        sample = {'volume': volume_sample, 'label': label_sample}
        
        return sample

class testDataset(Dataset): # test_dataset을 위해 조금 변형한 loader를 제작했습니다.
    
    def __init__ (self, test_args, transform=None):

        self.test_list = glob.glob (test_args.test_folder + '/*.png')
        self.test_list.sort()
        self.transform = transform
    
    def __len__(self):

        return len(self.test_list)
    
    def __getitem__(self, idx):
        
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        test_sample = plt.imread (self.test_list [idx])
        test_sample = test_sample.reshape ((1,)+test_sample.shape)
        
        volume_sample = torch.Tensor(test_sample)
        
        sample = {'test': test_sample}
        
        return sample

transform = transforms.Compose([transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_data = trainDataset(args, transform=transform) # dataset을 initialize할때 transform을 적용해줍니다.
test_data = testDataset(test_args, transform=transform)

train_dataloader = DataLoader(train_data, batch_size = args.batch_size,
                              num_workers=0, # broken pipe문제로 바꿨습니다.
                              shuffle=True)
test_dataloader = DataLoader(test_data, batch_size = test_args.batch_size,
                             num_workers=0,
                             shuffle=False)

"""## Choose Hyper-parameter"""

learning_rate = 0.001
num_epoch = 10
device = 'cuda'

"""## Construct a neural network builder



"""

class UNet(nn.Module):
  def __init__(self):
    super(UNet, self).__init__()

    self.encode1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(64),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(64),
                                 nn.ReLU(),
                                 )
    self.encode2 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(128),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(128),
                                 nn.ReLU(),
                                 )
    self.encode3 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(256),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(256),
                                 nn.ReLU(),
                                 )
    self.encode4 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(512),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(512),
                                 nn.ReLU(),
                                 )
    self.encode5 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(1024),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(1024),
                                 nn.ReLU(),
                                 )
    self.decode1 = nn.Sequential(nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(512),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(512),
                                 nn.ReLU(),
                                 )
    self.decode2 = nn.Sequential(nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(256),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(256),
                                 nn.ReLU(),
                                 )
    self.decode3 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(128),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(128),
                                 nn.ReLU(),
                                 )
    self.decode4 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(64),
                                 nn.ReLU(),
                                 nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),
                                 nn.BatchNorm2d(64),
                                 nn.ReLU(),
                                 )

    self.upsampling1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2,
                                stride=2)
    self.upsampling2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2,
                                stride=2)
    self.upsampling3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2,
                                stride=2)
    self.upsampling4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2,
                                stride=2)
    self.maxpooling = nn.MaxPool2d(kernel_size=2, stride=2)

    self.last = nn.Conv2d(in_channels=64, out_channels=1 , kernel_size=1, stride=1)


  def forward(self, x):

    # encoding
    x = self.encode1(x)
    x1 = x
    x = self.maxpooling(x)

    x = self.encode2(x)
    x2 = x
    x = self.maxpooling(x)

    x = self.encode3(x)
    x3 = x
    x = self.maxpooling(x)

    x = self.encode4(x)
    x4 = x
    x = self.maxpooling(x)

    x = self.encode5(x)

    # decoding
    x = self.upsampling1(x)
    x = torch.cat((x, x4), dim=1) # x의 shape를 출력해보고 붙일 차원을 정해야한다.
    x = self.decode1(x)
   
    x = self.upsampling2(x)
    x = torch.cat((x, x3), dim=1)
    x = self.decode2(x)
  

    x = self.upsampling3(x)
    x = torch.cat((x, x2), dim=1)
    x = self.decode3(x)
  

    x = self.upsampling4(x)
    x = torch.cat((x, x1), dim=1)
    x = self.decode4(x)
  
    x = self.last(x) # 차원만 down + feauture map을 표현하던 pixel을 label로 표현해주는 역할
    
    return x

"""## Initialize the network and optimizer"""

my_SegNet = UNet()
my_SegNet = my_SegNet.to(device)

optimizer = optim.Adam(my_SegNet.parameters(), lr = learning_rate)

"""## Load pre-trainde weights if exist"""

ckpt_dir = os.path.join('/gdrive/My Drive/Research/programming', 'checkpoints')
if not os.path.exists(ckpt_dir):
  os.makedirs(ckpt_dir)

lowest_loss = 1
ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')
if os.path.exists(ckpt_path):
  ckpt = torch.load(ckpt_path)
  try:
    my_SegNet.load_state_dict(ckpt['my_classifier'])
    optimizer.load_state_dict(ckpt['optimizer'])
    lowest_loss = ckpt['lowest_loss']
  except RuntimeError as e:
    print('wrong checkpoint')
  else:
    print('checkpoint is loaded !')
    print('current lowest_loss : %.2f' % lowest_loss)

"""## Train the network"""

i = 0
train_losses = []
loss_function = nn.BCEWithLogitsLoss() # 이진 분류의 경우에 사용하는 loss function, device 용량이 모자라서 to(device) 삭제

for epoch in range(num_epoch):
    # train phase
    my_SegNet.train()
    for batch, data in enumerate(train_dataloader): # dataloader를 사용해 보겠습니다.
        volume = data['volume'].to(device) # device 용량이 모자라서 to(device) 삭제
        label = data['label'].to(device) # device 용량이 모자라서 to(device) 삭제
        output = my_SegNet(volume)
        optimizer.zero_grad()
        loss = loss_function(output, label)
        loss.backward()
        optimizer.step()
        train_losses += [loss.item()]
    
    print('[epoch:{}] train loss : {:.4f}'.format(epoch, np.mean(train_losses)))

    if lowest_loss > np.mean(train_losses):
      lowest_loss = np.mean(train_losses)
      ckpt = {'my_classifier':my_SegNet.state_dict(),           
              'optimizer':optimizer.state_dict(),
              'lowest_loss':lowest_loss}
      torch.save(ckpt, ckpt_path)
      print('checkpoint is saved !')

"""## Test the network"""

# test data를 넣어서 시각화 시켜보는게 목적입니다. / test label이 없어 loss는 구할 수 없습니다.
# test를 할때 train network를 같이 돌려버리면 out of memory가 뜰 수 있습니다.
my_SegNet.load_state_dict(ckpt['my_classifier'])
torch.cuda.empty_cache()

my_SegNet.eval()
for batch, test_data in enumerate(test_dataloader):
  test = test_data['test'].to(device) 
  output = my_SegNet(test)
  break

plt.subplot(1, 2, 1)
test_volume = plt.imshow(test.squeeze().detach().cpu().numpy(), cmap='gray')
plt.subplot(1, 2, 2)
test_label = plt.imshow(output.squeeze().detach().cpu().numpy(), cmap='gray')


 if lowest_mgen_loss > mgen_loss:
    lowest_mgen_loss = mgen_loss
    M_ckpt = {'monet_generator':gen_p2m.state_dict(),           
              'optimizer':optimizerG.state_dict(),
              'mgen_loss':lowest_mgen_loss}
    torch.save(M_ckpt, M_ckpt_path)
    print('mgen checkpoint is saved !')

  if lowest_pgen_loss > pgen_loss:
    lowest_pgen_loss = pgen_loss
    P_ckpt = {'photo_generator':gen_m2p.state_dict(),                     
              'optimizer':optimizerG.state_dict(),
              'pgen_loss':lowest_pgen_loss}
    torch.save(P_ckpt, P_ckpt_path)
    print('pgen checkpoint is saved !')

  if lowest_mdis_loss > mdis_loss:
    lowest_mdis_loss = mdis_loss
    Md_ckpt = {'monet_discrimonator':m_dis.state_dict(),           
               'optimizer':optimizerD.state_dict(),
               'mgen_loss':lowest_mdis_loss}
    torch.save(Md_ckpt, Md_ckpt_path)
    print('mdis checkpoint is saved !')

  if lowest_pdis_loss > pdis_loss:
    lowest_pdis_loss = pdis_loss
    Pd_ckpt = {'photo_discrimonator':p_dis.state_dict(),             
               'optimizer':optimizerD.state_dict(),
               'mgen_loss':lowest_pdis_loss}
    torch.save(Pd_ckpt, Pd_ckpt_path)
    print('pdis checkpoint is saved !')